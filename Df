import hashlib
import json

def load_stored_hashes():
    client = bigquery.Client(credentials=credentials, project="steady-burner-336411")
    query = """
    SELECT dag_name, dependency_hash
    FROM `steady-burner-336411.vishal_dataset.dag_dependency_hashes`
    """
    query_job = client.query(query)
    result = query_job.result()
    stored_hashes = {row.dag_name: row.dependency_hash for row in result}
    return stored_hashes

def save_current_hashes(current_hashes):
    client = bigquery.Client(credentials=credentials, project="steady-burner-336411")
    table_id = 'steady-burner-336411.vishal_dataset.dag_dependency_hashes'

    rows_to_insert = []
    for dag_name, dependency_hash in current_hashes.items():
        rows_to_insert.append({'dag_name': dag_name, 'dependency_hash': dependency_hash})

    # Delete existing entries for these DAGs
    dag_names = [f"'{dag_name}'" for dag_name in current_hashes.keys()]
    delete_query = f"""
    DELETE FROM `{table_id}`
    WHERE dag_name IN ({', '.join(dag_names)})
    """
    client.query(delete_query).result()

    # Insert the new hashes
    errors = client.insert_rows_json(table_id, rows_to_insert)
    if errors:
        logger.error(f"Failed to save dependency hashes: {errors}")
    else:
        logger.info("Dependency hashes updated successfully.")
 def compute_dependency_hash(dependencies):
    # dependencies: a dictionary representing the DAG's dependencies
    dependencies_str = json.dumps(dependencies, sort_keys=True)
    return hashlib.md5(dependencies_str.encode('utf-8')).hexdigest()

def generate_dags():
    # ... existing code ...

    # Load stored hashes (from BigQuery or GCS)
    stored_hashes = load_stored_hashes()

    # Dictionary to hold current hashes
    current_hashes = {}

    # Generate a DAG file for each DAG in the dags dictionary
    for dag_name, data in dags.items():
        # Compute the current dependencies hash
        dependencies = {
            'common_queries': data['common_queries'],
            'acts': [{ 'act_id': act['act_id'], 'parent_node': act.get('parent_node', []) } for act in data['acts']]
        }
        current_hash = compute_dependency_hash(dependencies)
        current_hashes[dag_name] = current_hash

        # Get the stored hash for this DAG
        stored_hash = stored_hashes.get(dag_name)

        # Check if the dependencies have changed
        if current_hash != stored_hash:
            logger.info(f"Dependencies changed for DAG: {dag_name}. Regenerating DAG file.")
            # Proceed to generate and upload the DAG file
            # ... existing code to generate and upload DAG ...

        else:
            logger.info(f"No changes detected for DAG: {dag_name}. Skipping regeneration.")

    # After processing all DAGs, update the stored hashes
    save_current_hashes(current_hashes)


    
CREATE TABLE `steady-burner-336411.vishal_dataset.dag_dependency_hashes` (
    dag_name STRING NOT NULL,
    dependency_hash STRING NOT NULL
)
#####
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime
from google.cloud import storage, bigquery

# Function to download SQL file from GCS and execute it
def execute_sql_from_gcs(**kwargs):
    # Initialize GCS client
    storage_client = storage.Client()
    bucket_name = 'your-gcs-bucket-name'
    file_name = 'path/to/your/sqlfile.sql'

    # Get the bucket and blob (file)
    bucket = storage_client.get_bucket(bucket_name)
    blob = bucket.blob(file_name)
    
    # Download the SQL file content as a string
    sql_query = blob.download_as_text()

    # Initialize BigQuery client
    bq_client = bigquery.Client()

    # Run the SQL query
    query_job = bq_client.query(sql_query)

    # Wait for the query to finish
    query_job.result()

    print("SQL file executed successfully.")

# Define your DAG
default_args = {
    'owner': 'airflow',
    'start_date': datetime(2023, 9, 13),
    'retries': 1,
}

with DAG(dag_id='execute_sql_gcs_dag',
         default_args=default_args,
         schedule_interval=None,
         catchup=False) as dag:

    execute_sql_task = PythonOperator(
        task_id='execute_sql_from_gcs',
        python_callable=execute_sql_from_gcs,
        provide_context=True
    )
    
    execute_sql_task
