import hashlib
import json

def load_stored_hashes():
    client = bigquery.Client(credentials=credentials, project="steady-burner-336411")
    query = """
    SELECT dag_name, dependency_hash
    FROM `steady-burner-336411.vishal_dataset.dag_dependency_hashes`
    """
    query_job = client.query(query)
    result = query_job.result()
    stored_hashes = {row.dag_name: row.dependency_hash for row in result}
    return stored_hashes

def save_current_hashes(current_hashes):
    client = bigquery.Client(credentials=credentials, project="steady-burner-336411")
    table_id = 'steady-burner-336411.vishal_dataset.dag_dependency_hashes'

    rows_to_insert = []
    for dag_name, dependency_hash in current_hashes.items():
        rows_to_insert.append({'dag_name': dag_name, 'dependency_hash': dependency_hash})

    # Delete existing entries for these DAGs
    dag_names = [f"'{dag_name}'" for dag_name in current_hashes.keys()]
    delete_query = f"""
    DELETE FROM `{table_id}`
    WHERE dag_name IN ({', '.join(dag_names)})
    """
    client.query(delete_query).result()

    # Insert the new hashes
    errors = client.insert_rows_json(table_id, rows_to_insert)
    if errors:
        logger.error(f"Failed to save dependency hashes: {errors}")
    else:
        logger.info("Dependency hashes updated successfully.")
 def compute_dependency_hash(dependencies):
    # dependencies: a dictionary representing the DAG's dependencies
    dependencies_str = json.dumps(dependencies, sort_keys=True)
    return hashlib.md5(dependencies_str.encode('utf-8')).hexdigest()

def generate_dags():
    # ... existing code ...

    # Load stored hashes (from BigQuery or GCS)
    stored_hashes = load_stored_hashes()

    # Dictionary to hold current hashes
    current_hashes = {}

    # Generate a DAG file for each DAG in the dags dictionary
    for dag_name, data in dags.items():
        # Compute the current dependencies hash
        dependencies = {
            'common_queries': data['common_queries'],
            'acts': [{ 'act_id': act['act_id'], 'parent_node': act.get('parent_node', []) } for act in data['acts']]
        }
        current_hash = compute_dependency_hash(dependencies)
        current_hashes[dag_name] = current_hash

        # Get the stored hash for this DAG
        stored_hash = stored_hashes.get(dag_name)

        # Check if the dependencies have changed
        if current_hash != stored_hash:
            logger.info(f"Dependencies changed for DAG: {dag_name}. Regenerating DAG file.")
            # Proceed to generate and upload the DAG file
            # ... existing code to generate and upload DAG ...

        else:
            logger.info(f"No changes detected for DAG: {dag_name}. Skipping regeneration.")

    # After processing all DAGs, update the stored hashes
    save_current_hashes(current_hashes)


    
CREATE TABLE `steady-burner-336411.vishal_dataset.dag_dependency_hashes` (
    dag_name STRING NOT NULL,
    dependency_hash STRING NOT NULL
)
